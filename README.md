# Transformer model for NLP tasks
Simple setup for training differrent NLP models: language model (predicting next token), summarizer, classifier
using Transformer model

Summarizer uses pretrained embedding of BPE (byte-pair encoded) tokens


